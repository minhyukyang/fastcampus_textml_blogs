# 1. Part of speech tagging, Tokenization, and Out of vocabulary problem ([원문보기](https://lovit.github.io/nlp/2018/04/01/pos_and_oov/))
- lovit님의 글을 보고 개인적으로 정리한 내용입니다.

## 1.1 Tokenization, Part of speech tagging, Morphological analysis?
- 토크나이징 (tokenization) 은 주어진 문장을 토큰 (tokens) 으로 나누는 과정입니다. 토큰은 상황에 따라 다르게 정의할 수 있습니다. 띄어쓰기를 기준으로 문장을 나눌 수도 있습니다. 영어는 띄어쓰기를 기준으로 문장을 나눠도 단어열로 나뉘어집니다. 한국어에서 띄어쓰기 기준으로 나뉘어지는 단위는 ‘어절’ 입니다. 어절은 한 개 이상의 단어, 혹은 형태소로 구성될 수 있습니다. 
```
sent = '너무너무너무는 아이오아이의 노래입니다'
tokens = ['너무너무너무는', '아이오아이의', '노래입니다']

sent = 'Very-very-very is song of I.O.I'
tokens = ['Very-very-very', 'is', 'song', 'of', 'I.O.I']
```
- 품사 판별 (part of speech tagging)은 토큰을 (단어, 품사)로 정의합니다. 아래처럼 각 단어가 품사와 함께 분리되야 합니다.
```
tokens = [
    ('너무너무너무', '명사'),
    ('는', '조사'),
    ('아이오아이', '명사'),
    ('의', '조사'),
    ('노래', '명사'),
    ('입니다', '형용사')
]
```
- 형태소 분석(morphological analysis) : 품사 판별과 자주 혼동되는 개념, 형태소란 의미를 지니는 최소 단위로, (1) 자립형태소 / 의존형태소 로 나뉘기도 하며, (2) 실질형태소와 형식형태소로 나뉘기도 합니다.

데이터처리의 관점에서는 의미 부분과 문법기능 부분만 분리하면 됩니다. 이것만 제대로 된다면 의미부분만을 추릴 수 있고 (어절에서 명사+조사를 분리), 같은 의미를 지니는 단어들을 하나의 형태로 통합 (어근+어미를 원형으로 복원: 이었다 -> 이다) 할 수 있기 때문입니다.

## 1.2 Out of vocabulary problem
형태소 분석이나 품사 판별을 위해서는 사전과 문법이 필요합니다. ‘아이오아이는’ 이라는 어절이 ‘아이오아이는 = 아이오아이/명사 + 는/조사’ 라는 규칙을 알고 있지 않더라도 다음과 같은 사전과 문법 규칙을 지니고 있다면, ‘아이오아이는’ 이라는 어절을 인식할 수 있습니다.
```
dictionary = {
	'명사': {'아이오아이', '아이', '오'},
	'조사': {'는'}
}

pattern = [('명사', '조사'), ('명사',)]
```
KoNLPy (ver 0.4.4) 의 트위터 분석기를 이용하여 다음 문장의 품사 판별을 수행하였습니다.
```
from konlpy.tag import Twitter

twitter = Twitter()
twitter.pos('너무너무너무는 아이오아이의 노래입니다')

[('너무', 'Noun'),
 ('너무', 'Noun'),
 ('너무', 'Noun'),
 ('는', 'Josa'),
 ('아이오', 'Noun'),
 ('아이', 'Noun'),
 ('의', 'Josa'),
 ('노래', 'Noun'),
 ('입니', 'Adjective'),
 ('다', 'Eomi')]
 ```
일단 두 가지 큰 문제가 보입니다. ‘너무너무너무’가 ‘너무 + 너무 + 너무’로 나뉘어졌습니다. ‘아이오아이’는 ‘아이오 + 아이’로 나뉘어졌습니다. 트위터분석기에 ‘너무’, ‘아이오’, ‘아이’는 명사 사전에 등록이 되었지만, ‘너무너무너무’, ‘아이오아이’는 명사 사전에 존재하지 않기 때문입니다. 

또한 미등록단어 문제들은 주로 하나의 단어가 여러 개의 잘못된 단어로 나뉘어지는 형태로 발생합니다. 형태소 분석기, 품사 판별기는 학습 데이터를 바탕으로 주어진 문장을 이해합니다. 위와 같은 결과가 나온 이유는 학습 데이터를 이용한 모델에서 score(‘너무너무너무/unknown’ + ‘는/Josa’) < score(‘너무/Noun + 너무/Noun + 너무/Noun + 는/Josa’) 이기 때문입니다. 즉 학습데이터에서 보지 못했던 단어를 모르는 단어로 인식하는 것보다 아는 단어의 조합으로 나누는 것을 더 선호하기 떄문입니다.

이렇게 설계된 이유 중 하나는, KoNLPy 에 등록된 방법론들이 대부분 품사 판별기가 아닌 형태소 분석기이기 때문입니다. 각 엔진의 full name 입니다. 트위터 외의 모든 엔진은 분석기 입니다. Twitter 의 경우, 트위터의 hash tag 와 같은 국문법이 정의한 단어 이외의 다른 종류의 단어들을 포함하기 때문에 한국어 처리기라고 이름을 붙이지 않았을까 생각했습니다 (사견입니다).

```
Hannanum: 한나눔 형태소 분석기
Kkma: 꼬꼬마 형태소 분석기 (Kind Korean Moorpheme Analyzer)
Komoran: 코모란 한국어 형태소 분석기
Mecab: mecab-ko 형태소 분석기
Twitter: 트위터 한국어 처리기 -> 처리기!!
```
형태소 분석기의 목표는 단어를 형태소로 분해하는 것입니다. 그리고 형태소 분석을 통하여 품사 판별을 할 때의 가정 중 하나는 이전에 없던 형태소의 결합으로 신조어도 만들 수 있다는 점입니다. 예를 들어 ‘신/관형사’, ‘메뉴/명사’ 를 사전에 알고 있고, ‘관형사 + 명사 -> 명사’라는 규칙을 안다면 ‘신메뉴’를 명사로 인식할 수 있습니다.
```
사전: {신/관형사, 메뉴/명사}
형태소규칙: '관형사 + 명사 -> 명사'

pos('신메뉴') = morpheme(신/관형사 + 메뉴/명사) = '명사'
```

## 1.3 Ambiguity in tokenization
미등록단어 문제와 비슷한 문제가 또 있습니다. 누가 했던 말인지는 모르지만, 텍스트 데이터를 다루는 사람이라며 누구라도 할 수 있는 말입니다. ‘텍스트분석, 자연어처리는 처음부터 끝까지 **모호성과의 싸움** 입니다.
```
{
	'명사': {'서울', '서울대', '공원', '대공원'},
	'조사': {'에서'}
}
```
‘서울대/명사 + 공원/명사 + 에서/조사’도 가능하며, ‘서울/명사 + 대공원/명사 + 에서/조사’도 가능합니다.

형태소 분석기, 품사 판별기는 학습데이터를 바탕으로 가능한 여러 개우 후보 중에서 가장 가능성이 높은 후보를 최종 결과로 선택합니다. 만약 학습된 모델의 P(서울대 + 공원) > P(서울 + 대공원) 이었다면, 해당 어절은 ‘서울대 + 공원 + 에서’로 나뉘어질 것입니다. 설령 해당 문서가 어린이날 행사와 관련된 문서였더라도요.

학습 데이터의 도메인과 우리가 분석할 데이터의 도메인은 다를 가능성이 높습니다. 학습된 지식만을 이용하여 우리의 데이터를 분석하는 것은 ‘학습 데이터의 시선으로 우리의 데이터를 재단하는 것’입니다. 가르쳐 준대로만 행동합니다. 가르쳐 주지 않은 것도 스스로 배울 수 있으면 더 좋을 것입니다. 그렇기 때문에 우리에게 필요한 모델은 학습 데이터와 적용할 데이터 간의 차이를 이해하고, 이를 스스로에게 반영할 수 있는 모델입니다.

## 1.4 Needs for unsupervised word extractions
모든 품사에서 새로운 단어가 만들어지는 것은 아니라는 것입니다. 일단 명사는 새로운 단어가 자주 만들어짐을 압니다. 이는 새로운 개념이 만들어지기 때문입니다. 새로운 사건, 새로운 사람을 지칭하기 위하여 명사가 만들어집니다.

하지만 조사는 새롭게 만들어지지 않습니다. 조사는 문법 기능을 담당하기 때문입니다. 문법은 규칙이며, 규칙이 바뀔 때에는 이를 이용하는 사람들 간의 합의가 필요합니다. 그렇기 때문에 조사와 같은 문법이 변하는데는 긴 시간이 필요합니다.

새로운 어미도 만들어집니다. 말투 때문입니다. ‘저녁 이제 먹을라궁’ 처럼 대화체에서는 ‘-을라궁’ 처럼 다양한 어미가 만들어지기도 합니다. 일단 어떤 품사에서 새로운 단어가 만들어지는지를 파악해야 그 방법을 설계할 수 있습니다.

규칙 기반의 방법이 새로운 단어를 인식하는데 이용될 수도 있습니다만, 이는 매우 위험한 접근입니다. ‘-은/조사’는 명사 뒤에 등장합니다. 이 규칙을 이용하여 ‘-은’ 앞을 명사라고 생각할 수도 있습니다. 그러나 에이핑크의 ‘손나은’ 은 ‘손나/명사 + 은/조사’가 아닙니다. 한국어 텍스트에 자주 이용되는 글자수는 1천자가 되지 않습니다. 뉴스와 같은 텍스트에서는 300 자 정도로 99 % 이상의 단어가 기술됩니다. ‘은’으로 끝나는 수 많은 명사가 존재합니다.

다른 해결 방법으로 통계를 이용한 단어 추출 방법들이 있습니다. 단어의 경계에는 특징이 있습니다. 예를 들어 ‘손나은’ 다음에 등장하는 글자는 ‘손나은 + 은’, ‘손나은 + 이’, ‘손나은 + 의’ 처럼 다양합니다. 또한 연예 뉴스에서는 ‘손나은’ 이라는 세글자는 자주 등장합니다. 그렇기 때문에 ‘손나’라는 두 단어 다음에 등장할 글자는 ‘은’으로 예상이 됩니다. 하지만 ‘손나은’ 다음에 등장할 글자는 모두가 다르게 예상할 것입니다. 이러한 특징을 이용하여 단어를 추출하는 방법들을 이후에 이야기하려 합니다.

### Related posts
- [Left-side subword tokenizer](https://lovit.github.io/nlp/2018/04/02/simplest_tokenizers/) 는 문서 판별 등의 작업에 이용할 수 있는 아주 간단한 토크나이저 입니다.
- [Word piece model](https://lovit.github.io/nlp/2018/04/02/wpm/) 은 out of vocabulary 문제를 우회 (해결이 아닙니다) 하는 토크나이저 입니다.
- [Cohesion score](https://lovit.github.io/nlp/2018/04/09/cohesion_ltokenizer/) 는 단어의 일부분으로 다른 부분이 얼마나 잘 예상되느냐에 대한 정보를 단어 추출에 이용합니다.
- [KR-WordRank](https://lovit.github.io/nlp/2018/04/16/krwordrank/) 는 graph ranking 방법을 이용하여 단어를 추출합니다.
- [Branching Entropy](https://lovit.github.io/nlp/2018/04/09/branching_entropy_accessor_variety/) 와 [Acessor Variety](https://lovit.github.io/nlp/2018/04/09/branching_entropy_accessor_variety/) 는 손나은의 오른쪽에 등장하는 글자의 다양성의 정보를 이용합니다.

